{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "from utils import complete_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plain_wikipedia_file = 'plain_wikipedia.txt'\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=5000)\n",
    "tokenizer.train(files=[plain_wikipedia_file], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are loading the Neural Network for khmer that we already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = GPT2LMHeadModel.from_pretrained(\"gpt2-wikipedia-khmer-no-pretrain\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-wikipedia-khmer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the models to complete sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training\n",
      "á”áŸ’ášá¶áŸá¶á‘  á¢á„áŸ’á‚ áš áœááŸ’á á‚áŸ’ášá¼ åˆ¥ åˆ¥ åˆ¥ åˆ¥ åˆ¥ åˆ¥ åˆ¥ åˆ¥ åˆ¥ åˆ¥ Ë§ Ë§ Ë§ Ë§ Ë§ Ë§ Ë§ Ë§ é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª Ë§ Ë§ Ë§ Ë§ Ë§ Ë§ Ë§ é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª é˜ª á™á€ á™á€ á™á€ á™á€ á™á€ á™á€ á™á€ á™á€ á™á€ æœ áŸ’á¢á¼á“ áŸ’á¢á¼á“ áŸ’á¢á¼á“\n",
      "After training\n",
      "á”áŸ’ášá¶áŸá¶á‘  á¢á„áŸ’á‚ áš áœááŸ’á  ( á¢á„áŸ‹á‚áŸ’á›áŸáŸ :  B or om   R e a ch e a  I I )  ( á”áŸ’áš .áŸ | á‚.áŸ   áŸ áŸ  áŸ áŸ  - áŸ¡ áŸ áŸ  áŸ¢ )  áš á‡áŸ’á‡ á€á¶á› á‚áŸ’áš á„ ášá¶á‡  ( á‚.áŸ  áŸ¡ áŸ£ áŸ  - áŸ¡ áŸ¡áŸ¦ áŸ  )  á€áŸ’ášáŸ„á™ á‚áŸ’áš á„ ášá¶á‡ áŸá˜áŸ’á”ááŸ’áá· á•áŸ’á›á¼áœ á€á¶ášááŸ á€áŸ’á“á»á„ á†áŸ’á“á¶áŸ† áŸ¡\n"
     ]
    }
   ],
   "source": [
    "sentence_to_complete = \"á”áŸ’ášá¶áŸá¶á‘ á¢á„áŸ’á‚ášáœááŸ’á\" # Angkor Wat\n",
    "print(\"Before training\")\n",
    "complete_text(sentence_to_complete, initial_model, tokenizer)\n",
    "print(\"After training\")\n",
    "complete_text(sentence_to_complete, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training\n",
      "á”áŸ’ášá‘áŸáŸá€á˜áŸ’á–á»á‡á¶ åœˆ åœˆ åœˆ åœˆ åœˆ åœˆ åœˆ åœˆ âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš âˆš ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª ç¦ª\n",
      "After training\n",
      "á”áŸ’ášá‘áŸáŸá€á˜áŸ’á–á»á‡á¶ á‚áºá‡á¶ á”áŸ’ášá‘áŸáŸ á˜á½á™ áŠáŸ‚á› áŸáŸ’áá·áá“áŸ… ááŸ†á”á“áŸ‹ á¢á¶áŸáŸŠá¸ á¢á¶ á‚áŸ’á“ áŸá™ áŸ áŠá¸ á‚ áŸ„á€ áŸ”  á”áŸ’ášá‘áŸáŸ á“áŸáŸ‡á˜á¶á“ á–áŸ’ášáŸ†á”áŸ’ášá‘á›áŸ‹ á‡á¶á”áŸ‹ á“ á¹á„  á” á¹á„ á‘á“áŸ’á› áŸáŸ á¶á”  á“á·á„ ááŸááŸ’á áš á á“ á‚ á· ášá¸  á“á·á„ ááŸááŸ’á áš á á“ á‚ á· ášá¸ áŸ”  á” á¹á„ á‘á“áŸ’á› áŸáŸ á¶á” á˜á¶á“ á–áŸ’ášáŸ†á”áŸ’ášá‘á›áŸ‹ á‡á¶á”áŸ‹ ááŸááŸ’á áš á á“ á‚ á· ášá¸  á“á·á„ ááŸááŸ’á áš á á“\n"
     ]
    }
   ],
   "source": [
    "sentence_to_complete = \"á”áŸ’ášá‘áŸáŸá€á˜áŸ’á–á»á‡á¶\" # Cambodia country\n",
    "print(\"Before training\")\n",
    "complete_text(sentence_to_complete, initial_model, tokenizer)\n",
    "print(\"After training\")\n",
    "complete_text(sentence_to_complete, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training\n",
      "á‡áŸ† ášá¶á” áŸá½áš oc oc oc oc oc oc oc oc oc oc oc oc oc áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ› áˆ›\n",
      "After training\n",
      "á‡áŸ† ášá¶á” áŸá½áš . á á» .  á’ . ) á á» .  á’ . á á» .  á’ . á á» .  á’ . á á» .  á’ .\n"
     ]
    }
   ],
   "source": [
    "sentence_to_complete = \"á‡áŸ†ášá¶á”áŸá½áš\" # Hello\n",
    "print(\"Before training\")\n",
    "complete_text(sentence_to_complete, initial_model, tokenizer)\n",
    "print(\"After training\")\n",
    "complete_text(sentence_to_complete, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training\n",
      "-5.834786 á á¾  1 + 1  á‡á¶ á¢áŸ’áœá¸ ? ç¾… ç¾… ç¾… ç¾… ç¾… ç¾… ç¾… ç¾… ç¾… ç¾… ç¾… ç¾… ç¾… è•­ è•­ è•­ è•­ è•­ è•­ è•­ è•­ è•­ ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  ğ­  å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥ å“¥\n",
      "After training\n",
      "-0.4089401 á á¾  1 + 1  á‡á¶ á¢áŸ’áœá¸ ? á á» .  á’ . ) á á» .  á’ . ,  á á» .  á’ . ,  á á» .  á‡á¶ .  á‘ áŸ á€ . ,  á á» .  á’ .\n"
     ]
    }
   ],
   "source": [
    "sentence_to_complete = \"áá¾ 1+1 á‡á¶á¢áŸ’áœá¸?\" # What is 1+1?\n",
    "print(\"Before training\")\n",
    "complete_text(sentence_to_complete, initial_model, tokenizer)\n",
    "print(\"After training\")\n",
    "complete_text(sentence_to_complete, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-khmer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
