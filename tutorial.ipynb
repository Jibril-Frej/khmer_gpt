{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from transformers import AdamW, GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_wikipedia_file = 'text/AA/wiki_00' \n",
    "plain_wikipedia_file = 'plain_wikipedia.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the json file and write the article title and text into a single text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(plain_wikipedia_file, 'w') as f:\n",
    "    for line in open(json_wikipedia_file, 'r', encoding='utf-8'):\n",
    "        article = json.loads(line)\n",
    "        f.write(article['title'])\n",
    "        f.write('\\n')\n",
    "        f.write(article['text'])\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Tokenizer: it is the object that will analyse all the text and build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the text using the tikenizer to get the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files=[plain_wikipedia_file], trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3632, 4960, 1138, 3391]\n",
      "['ប្រាសាទ', ' អង្គ', 'រ', 'វត្ត']\n"
     ]
    }
   ],
   "source": [
    "sentence_to_complete = \"ប្រាសាទ អង្គរវត្ត\"\n",
    "\n",
    "output = tokenizer.encode(sentence_to_complete)\n",
    "print(output.ids)\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ប្រាសាទ  អង្គ រ វត្ត\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output.ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170392it [00:11, 14380.72it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "with open(plain_wikipedia_file, 'r', encoding='utf-8') as file:\n",
    "    for line in tqdm(file):\n",
    "        tokenized_line = tokenizer.encode(line.strip()).ids\n",
    "        if tokenized_line:\n",
    "            tokenized_texts.append(tokenized_line[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tokenized_texts[idx])\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Pad the sequences in the batch\n",
    "    batch_padded = pad_sequence([sequence for sequence in batch], \n",
    "                                batch_first=True, padding_value=tokenizer.token_to_id(\"[PAD]\"))\n",
    "    # Create attention masks\n",
    "    attention_masks = torch.zeros(batch_padded.shape, dtype=torch.long)\n",
    "    attention_masks[batch_padded != tokenizer.token_to_id(\"[PAD]\")] = 1\n",
    "\n",
    "    return batch_padded, attention_masks\n",
    "\n",
    "\n",
    "dataset = TextDataset(tokenized_texts)\n",
    "data_loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(vocab_size=len(tokenizer.get_vocab()), n_positions=32, n_layer=4, n_head=4, n_embed=64, pad_token_id = tokenizer.token_to_id(\"[PAD]\"))\n",
    "model = GPT2LMHeadModel(config)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត I I I I b b b b b b ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី \n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត I I I I b b b b b b ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ຸ\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត I I I I b b b b b b ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  គ្រប់គ្រ\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត I I I I b b b b b b ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  \n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត I I I I b b b b b b ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ី  ଼\n"
     ]
    }
   ],
   "source": [
    "input_text = sentence_to_complete\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text).ids]).to(device)\n",
    "attention_mask = torch.ones_like(input_ids).to(device)\n",
    "max_length = 32  # Generating 5 additional tokens\n",
    "model.eval()\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_beams=5, num_return_sequences=5).to(device)\n",
    "for prediction in output:\n",
    "    predicted_text = tokenizer.decode(prediction.tolist(), skip_special_tokens=True)\n",
    "    print(\"Predicted text:\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0/12970, Loss: 8.368127822875977\n",
      "Epoch: 0, Batch: 100/12970, Loss: 4.9140119552612305\n",
      "Epoch: 0, Batch: 200/12970, Loss: 4.436372756958008\n",
      "Epoch: 0, Batch: 300/12970, Loss: 4.169074535369873\n",
      "Epoch: 0, Batch: 400/12970, Loss: 3.6056904792785645\n",
      "Epoch: 0, Batch: 500/12970, Loss: 3.4483678340911865\n",
      "Epoch: 0, Batch: 600/12970, Loss: 3.5918898582458496\n",
      "Epoch: 0, Batch: 700/12970, Loss: 3.2188501358032227\n",
      "Epoch: 0, Batch: 800/12970, Loss: 3.2799620628356934\n",
      "Epoch: 0, Batch: 900/12970, Loss: 3.091048240661621\n",
      "Epoch: 0, Batch: 1000/12970, Loss: 3.4663119316101074\n",
      "Epoch: 0, Batch: 1100/12970, Loss: 3.3963613510131836\n",
      "Epoch: 0, Batch: 1200/12970, Loss: 2.946043014526367\n",
      "Epoch: 1, Batch: 1300/12970, Loss: 2.806199550628662\n",
      "Epoch: 1, Batch: 1400/12970, Loss: 2.561195135116577\n",
      "Epoch: 1, Batch: 1500/12970, Loss: 2.9467434883117676\n",
      "Epoch: 1, Batch: 1600/12970, Loss: 3.0001707077026367\n",
      "Epoch: 1, Batch: 1700/12970, Loss: 2.7308874130249023\n",
      "Epoch: 1, Batch: 1800/12970, Loss: 2.953626871109009\n",
      "Epoch: 1, Batch: 1900/12970, Loss: 2.7206063270568848\n",
      "Epoch: 1, Batch: 2000/12970, Loss: 2.9666738510131836\n",
      "Epoch: 1, Batch: 2100/12970, Loss: 2.933473587036133\n",
      "Epoch: 1, Batch: 2200/12970, Loss: 2.675896644592285\n",
      "Epoch: 1, Batch: 2300/12970, Loss: 2.723212242126465\n",
      "Epoch: 1, Batch: 2400/12970, Loss: 2.442401170730591\n",
      "Epoch: 1, Batch: 2500/12970, Loss: 2.681178569793701\n",
      "Epoch: 2, Batch: 2600/12970, Loss: 2.582677125930786\n",
      "Epoch: 2, Batch: 2700/12970, Loss: 2.8589541912078857\n",
      "Epoch: 2, Batch: 2800/12970, Loss: 2.829192876815796\n",
      "Epoch: 2, Batch: 2900/12970, Loss: 2.6272568702697754\n",
      "Epoch: 2, Batch: 3000/12970, Loss: 2.4492297172546387\n",
      "Epoch: 2, Batch: 3100/12970, Loss: 2.619137763977051\n",
      "Epoch: 2, Batch: 3200/12970, Loss: 2.7197036743164062\n",
      "Epoch: 2, Batch: 3300/12970, Loss: 2.6139047145843506\n",
      "Epoch: 2, Batch: 3400/12970, Loss: 2.6247055530548096\n",
      "Epoch: 2, Batch: 3500/12970, Loss: 2.2780568599700928\n",
      "Epoch: 2, Batch: 3600/12970, Loss: 2.5776491165161133\n",
      "Epoch: 2, Batch: 3700/12970, Loss: 2.5042309761047363\n",
      "Epoch: 2, Batch: 3800/12970, Loss: 2.3755059242248535\n",
      "Epoch: 3, Batch: 3900/12970, Loss: 2.5331790447235107\n",
      "Epoch: 3, Batch: 4000/12970, Loss: 2.2836711406707764\n",
      "Epoch: 3, Batch: 4100/12970, Loss: 2.6855764389038086\n",
      "Epoch: 3, Batch: 4200/12970, Loss: 2.4245333671569824\n",
      "Epoch: 3, Batch: 4300/12970, Loss: 2.1877119541168213\n",
      "Epoch: 3, Batch: 4400/12970, Loss: 2.082475423812866\n",
      "Epoch: 3, Batch: 4500/12970, Loss: 2.2836198806762695\n",
      "Epoch: 3, Batch: 4600/12970, Loss: 2.548135280609131\n",
      "Epoch: 3, Batch: 4700/12970, Loss: 2.717278242111206\n",
      "Epoch: 3, Batch: 4800/12970, Loss: 2.305281162261963\n",
      "Epoch: 3, Batch: 4900/12970, Loss: 2.4922361373901367\n",
      "Epoch: 3, Batch: 5000/12970, Loss: 2.2938883304595947\n",
      "Epoch: 3, Batch: 5100/12970, Loss: 2.437584400177002\n",
      "Epoch: 4, Batch: 5200/12970, Loss: 2.0693447589874268\n",
      "Epoch: 4, Batch: 5300/12970, Loss: 2.1624135971069336\n",
      "Epoch: 4, Batch: 5400/12970, Loss: 2.2417221069335938\n",
      "Epoch: 4, Batch: 5500/12970, Loss: 2.247274875640869\n",
      "Epoch: 4, Batch: 5600/12970, Loss: 2.352806806564331\n",
      "Epoch: 4, Batch: 5700/12970, Loss: 2.37626576423645\n",
      "Epoch: 4, Batch: 5800/12970, Loss: 2.3156490325927734\n",
      "Epoch: 4, Batch: 5900/12970, Loss: 2.3676364421844482\n",
      "Epoch: 4, Batch: 6000/12970, Loss: 2.3890552520751953\n",
      "Epoch: 4, Batch: 6100/12970, Loss: 2.3041646480560303\n",
      "Epoch: 4, Batch: 6200/12970, Loss: 2.1486573219299316\n",
      "Epoch: 4, Batch: 6300/12970, Loss: 2.1564621925354004\n",
      "Epoch: 4, Batch: 6400/12970, Loss: 1.9488247632980347\n",
      "Epoch: 5, Batch: 6500/12970, Loss: 1.9506679773330688\n",
      "Epoch: 5, Batch: 6600/12970, Loss: 2.0533313751220703\n",
      "Epoch: 5, Batch: 6700/12970, Loss: 2.068521022796631\n",
      "Epoch: 5, Batch: 6800/12970, Loss: 2.4084408283233643\n",
      "Epoch: 5, Batch: 6900/12970, Loss: 1.9744138717651367\n",
      "Epoch: 5, Batch: 7000/12970, Loss: 1.9382355213165283\n",
      "Epoch: 5, Batch: 7100/12970, Loss: 2.2876968383789062\n",
      "Epoch: 5, Batch: 7200/12970, Loss: 1.845794677734375\n",
      "Epoch: 5, Batch: 7300/12970, Loss: 2.340355634689331\n",
      "Epoch: 5, Batch: 7400/12970, Loss: 2.500826597213745\n",
      "Epoch: 5, Batch: 7500/12970, Loss: 2.196010112762451\n",
      "Epoch: 5, Batch: 7600/12970, Loss: 2.013122320175171\n",
      "Epoch: 5, Batch: 7700/12970, Loss: 2.147145986557007\n",
      "Epoch: 6, Batch: 7800/12970, Loss: 2.2358460426330566\n",
      "Epoch: 6, Batch: 7900/12970, Loss: 2.168672561645508\n",
      "Epoch: 6, Batch: 8000/12970, Loss: 2.1527092456817627\n",
      "Epoch: 6, Batch: 8100/12970, Loss: 2.187319755554199\n",
      "Epoch: 6, Batch: 8200/12970, Loss: 2.2199857234954834\n",
      "Epoch: 6, Batch: 8300/12970, Loss: 1.9668998718261719\n",
      "Epoch: 6, Batch: 8400/12970, Loss: 2.3684194087982178\n",
      "Epoch: 6, Batch: 8500/12970, Loss: 2.1310150623321533\n",
      "Epoch: 6, Batch: 8600/12970, Loss: 2.3519697189331055\n",
      "Epoch: 6, Batch: 8700/12970, Loss: 2.170083522796631\n",
      "Epoch: 6, Batch: 8800/12970, Loss: 1.984673261642456\n",
      "Epoch: 6, Batch: 8900/12970, Loss: 2.087447166442871\n",
      "Epoch: 6, Batch: 9000/12970, Loss: 2.2451624870300293\n",
      "Epoch: 7, Batch: 9100/12970, Loss: 2.000290870666504\n",
      "Epoch: 7, Batch: 9200/12970, Loss: 1.9237580299377441\n",
      "Epoch: 7, Batch: 9300/12970, Loss: 1.9035018682479858\n",
      "Epoch: 7, Batch: 9400/12970, Loss: 2.259476900100708\n",
      "Epoch: 7, Batch: 9500/12970, Loss: 2.1058266162872314\n",
      "Epoch: 7, Batch: 9600/12970, Loss: 1.889522671699524\n",
      "Epoch: 7, Batch: 9700/12970, Loss: 2.1053578853607178\n",
      "Epoch: 7, Batch: 9800/12970, Loss: 2.138861894607544\n",
      "Epoch: 7, Batch: 9900/12970, Loss: 1.9102518558502197\n",
      "Epoch: 7, Batch: 10000/12970, Loss: 2.014462471008301\n",
      "Epoch: 7, Batch: 10100/12970, Loss: 2.123652696609497\n",
      "Epoch: 7, Batch: 10200/12970, Loss: 1.8984425067901611\n",
      "Epoch: 7, Batch: 10300/12970, Loss: 1.9342526197433472\n",
      "Epoch: 8, Batch: 10400/12970, Loss: 1.7551460266113281\n",
      "Epoch: 8, Batch: 10500/12970, Loss: 1.8311831951141357\n",
      "Epoch: 8, Batch: 10600/12970, Loss: 2.008303165435791\n",
      "Epoch: 8, Batch: 10700/12970, Loss: 1.9393028020858765\n",
      "Epoch: 8, Batch: 10800/12970, Loss: 1.8622502088546753\n",
      "Epoch: 8, Batch: 10900/12970, Loss: 1.7958903312683105\n",
      "Epoch: 8, Batch: 11000/12970, Loss: 1.9765503406524658\n",
      "Epoch: 8, Batch: 11100/12970, Loss: 1.9513201713562012\n",
      "Epoch: 8, Batch: 11200/12970, Loss: 1.8979172706604004\n",
      "Epoch: 8, Batch: 11300/12970, Loss: 1.952120065689087\n",
      "Epoch: 8, Batch: 11400/12970, Loss: 2.1570677757263184\n",
      "Epoch: 8, Batch: 11500/12970, Loss: 1.9289896488189697\n",
      "Epoch: 8, Batch: 11600/12970, Loss: 2.085845470428467\n",
      "Epoch: 9, Batch: 11700/12970, Loss: 2.0122461318969727\n",
      "Epoch: 9, Batch: 11800/12970, Loss: 1.9126787185668945\n",
      "Epoch: 9, Batch: 11900/12970, Loss: 1.7580995559692383\n",
      "Epoch: 9, Batch: 12000/12970, Loss: 1.972915530204773\n",
      "Epoch: 9, Batch: 12100/12970, Loss: 1.9339145421981812\n",
      "Epoch: 9, Batch: 12200/12970, Loss: 1.7978097200393677\n",
      "Epoch: 9, Batch: 12300/12970, Loss: 1.8373535871505737\n",
      "Epoch: 9, Batch: 12400/12970, Loss: 1.9180527925491333\n",
      "Epoch: 9, Batch: 12500/12970, Loss: 1.9869654178619385\n",
      "Epoch: 9, Batch: 12600/12970, Loss: 2.0039632320404053\n",
      "Epoch: 9, Batch: 12700/12970, Loss: 1.8317774534225464\n",
      "Epoch: 9, Batch: 12800/12970, Loss: 1.798796534538269\n",
      "Epoch: 9, Batch: 12900/12970, Loss: 1.8476812839508057\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "batch_id = 0\n",
    "max_epoch = 10\n",
    "for epoch in range(max_epoch):\n",
    "    for batch, attention_mask in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_id}/{len(data_loader)*max_epoch}, Loss: {loss.item()}')\n",
    "        batch_id += 1\n",
    "        # if batch_id % 500 == 0:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ កសាង ឡើង ក្នុង\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ  ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ កសាង ឡើង ក្នុង\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ កសាង ឡើង នៅ\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ  ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ កសាង ឡើង នៅ\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ  ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ ត្រូវបាន កសាង ឡើង\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ ត្រូវបាន កសាង ឡើង\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ កសាង ឡើង ដោយ\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ  ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ កសាង ឡើង ដោយ\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ ដែល កសាង ឡើង ដោយ ព្រះបាទ សូរ ្យ វរ្ម័ន ទី២ ។  ប្រាសាទ នេះ កសាង ឡើង នៅ សត វត្សរ៍ ទី១ ២ ។  ប្រាសាទ នេះ\n",
      "Predicted text: ប្រាសាទ  អង្គ រ វត្ត  ជា ស្ថាប ត ្យ កម្ម ខ្មែរ  ដែល កសាង ឡើង ដោយ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧  ក្នុង រាជ ព្រះបាទ ជ័យ វរ្ម័ន ទី ៧ ។  ប្រាសាទ នេះ ស្ថិតនៅ ច ន្ល\n"
     ]
    }
   ],
   "source": [
    "sentence_to_complete = \"ប្រាសាទ អង្គរវត្ត\"\n",
    "input_text = sentence_to_complete\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text).ids]).to(device)\n",
    "attention_mask = torch.ones_like(input_ids).to(device)\n",
    "max_length = 32  # Generating 5 additional tokens\n",
    "model.eval()\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_beams=10, num_return_sequences=10).to(device)\n",
    "for prediction in output:\n",
    "    predicted_text = tokenizer.decode(prediction.tolist(), skip_special_tokens=True)\n",
    "    print(\"Predicted text:\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង ក្នុង\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះមាន ច ម្ ងាយ\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ ស្ថិតនៅ ច ម្\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅ ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង ក្នុង\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង នៅ\n",
      "Predicted text: ប្រាសាទ នាង ខ្ម ៅ មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង ក្នុង សម័យ ច េន\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ ត្រូវបាន កសាង ឡើង\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះមាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង ដោយ\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅ ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។\n"
     ]
    }
   ],
   "source": [
    "sentence_to_complete = \"ប្រាសាទ\"\n",
    "input_text = sentence_to_complete\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text).ids]).to(device)\n",
    "attention_mask = torch.ones_like(input_ids).to(device)\n",
    "max_length = 32  # Generating 5 additional tokens\n",
    "model.eval()\n",
    "output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_beams=10, num_return_sequences=10).to(device)\n",
    "for prediction in output:\n",
    "    predicted_text = tokenizer.decode(prediction.tolist(), skip_special_tokens=True)\n",
    "    print(\"Predicted text:\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gpt2-wikipedia-khmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = GPT2LMHeadModel.from_pretrained(\"gpt2-wikipedia-khmer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង ក្នុង\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះមាន ច ម្ ងាយ\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ ស្ថិតនៅ ច ម្\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅ ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង ក្នុង\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង នៅ\n",
      "Predicted text: ប្រាសាទ នាង ខ្ម ៅ មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង ក្នុង សម័យ ច េន\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ ត្រូវបាន កសាង ឡើង\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះមាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅក្នុង ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។  ប្រាសាទ នេះ កសាង ឡើង ដោយ\n",
      "Predicted text: ប្រាសាទ ស ំប ូរ ព្រៃ គ ុក មាន ទីតាំង ស្ថិតនៅ ភូមិ ស ំប ូរ  ឃុំ ស ំប ូរ  ស្រុក ប្រាសាទ ស ំប ូរ  ខេត្ត កំពង់ ធំ ។\n"
     ]
    }
   ],
   "source": [
    "sentence_to_complete = \"ប្រាសាទ\"\n",
    "input_text = sentence_to_complete\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text).ids])\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "max_length = 32  # Generating 5 additional tokens\n",
    "new_model.eval()\n",
    "output = new_model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_beams=10, num_return_sequences=10).to(device)\n",
    "for prediction in output:\n",
    "    predicted_text = tokenizer.decode(prediction.tolist(), skip_special_tokens=True)\n",
    "    print(\"Predicted text:\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"gpt2-wikipedia-khmer-tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-khmer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
